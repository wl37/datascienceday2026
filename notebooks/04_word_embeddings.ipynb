{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8c3e15bf",
      "metadata": {},
      "source": [
        "# ðŸ—£ï¸ Word Embeddings\n",
        "## *Teaching computers what words mean*\n",
        "\n",
        "---\n",
        "\n",
        "How does ChatGPT know that **\"dog\"** and **\"puppy\"** are similar?\n",
        "\n",
        "Or that **\"Paris\"** is to **\"France\"** as **\"Tokyo\"** is to **\"Japan\"**?\n",
        "\n",
        "The answer: **word embeddings** â€” representing every word as a list of numbers (a *vector*) so that words with similar meanings end up close together in space.\n",
        "\n",
        "```\n",
        "king   â†’ [0.32, -0.51, 0.78, 0.14, ...50 numbers...]\n",
        "queen  â†’ [0.30, -0.48, 0.75, 0.19, ...]\n",
        "dog    â†’ [-0.62, 0.33, -0.10, 0.88, ...]\n",
        "```\n",
        "\n",
        "This is the **foundation of all modern AI language models** â€” ChatGPT, Gemini, Claude, all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a53833e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as pe\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "sns.set_theme(style=\"white\", font_scale=1.15)\n",
        "\n",
        "# Load the pre-sampled GloVe-style vectors\n",
        "df = pd.read_csv(\"../data/glove_sample.csv\")\n",
        "words      = df[\"word\"].tolist()\n",
        "categories = df[\"category\"].tolist()\n",
        "vectors    = df.drop(columns=[\"word\", \"category\"]).values  # shape: (80, 50)\n",
        "\n",
        "print(f\"Loaded {len(words)} words across {df['category'].nunique()} categories.\")\n",
        "print(\"Categories:\", sorted(df[\"category\"].unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac6ecbf1",
      "metadata": {},
      "source": [
        "## What is a Vector?\n",
        "\n",
        "Think of it as **coordinates** for meaning.\n",
        "\n",
        "In 2D, we can place words on a map. In 50D, we can capture much richer relationships.\n",
        "\n",
        "**Cosine similarity** measures how *similar* two word-vectors point:\n",
        "- Score **1.0** = identical direction (same meaning)\n",
        "- Score **0.0** = perpendicular (unrelated)\n",
        "- Score **-1.0** = opposite direction (antonyms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58f9e77d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:\n",
        "    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
        "\n",
        "\n",
        "def get_vector(word: str) -> np.ndarray:\n",
        "    idx = words.index(word)\n",
        "    return vectors[idx]\n",
        "\n",
        "\n",
        "def nearest_neighbors(word: str, n: int = 5) -> list[tuple[str, float]]:\n",
        "    v   = get_vector(word)\n",
        "    sims = [(w, cosine_similarity(v, vectors[i]))\n",
        "            for i, w in enumerate(words) if w != word]\n",
        "    return sorted(sims, key=lambda x: x[1], reverse=True)[:n]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3f93b15",
      "metadata": {},
      "source": [
        "## Interactive Demo 1 â€” Nearest Neighbors\n",
        "\n",
        "Pick a word and see which other words are most similar to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a7bb6d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "@widgets.interact(\n",
        "    word=widgets.Dropdown(options=sorted(words), value=\"king\", description=\"Word:\",\n",
        "                          layout=widgets.Layout(width=\"250px\")),\n",
        "    n=widgets.IntSlider(value=5, min=3, max=10, step=1, description=\"Show top:\",\n",
        "                        style={\"description_width\": \"initial\"},\n",
        "                        layout=widgets.Layout(width=\"300px\")),\n",
        ")\n",
        "def show_neighbors(word, n):\n",
        "    neighbors = nearest_neighbors(word, n)\n",
        "    neighbor_words  = [w for w, _ in neighbors]\n",
        "    neighbor_scores = [s for _, s in neighbors]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, max(3, n * 0.55)))\n",
        "\n",
        "    colors = plt.cm.RdYlGn(np.array(neighbor_scores))\n",
        "    bars = ax.barh(range(n), neighbor_scores, color=colors, edgecolor=\"white\", linewidth=1)\n",
        "    ax.set_yticks(range(n))\n",
        "    ax.set_yticklabels(neighbor_words, fontsize=12)\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_xlabel(\"Cosine Similarity\")\n",
        "    ax.set_title(f'Words most similar to \"{word}\"', fontsize=14, weight=\"bold\")\n",
        "    ax.set_xlim(0, 1.05)\n",
        "    ax.bar_label(bars, fmt=\"%.3f\", padding=4, fontsize=11, fontweight=\"bold\")\n",
        "\n",
        "    cat = categories[words.index(word)]\n",
        "    ax.set_xlabel(f'Cosine Similarity   (category: {cat})', fontsize=11)\n",
        "\n",
        "    sns.despine(left=True, bottom=True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24b216a5",
      "metadata": {},
      "source": [
        "## Interactive Demo 2 â€” Word Map (PCA)\n",
        "\n",
        "50 dimensions is impossible to visualize. We use **PCA (Principal Component Analysis)** to compress\n",
        "50 dimensions down to 2 while preserving as much structure as possible.\n",
        "\n",
        "Words with similar meanings should cluster together on the map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e3247fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_palette = {\n",
        "    \"royalty\":   \"#E8575A\",\n",
        "    \"animals\":   \"#5B8FB9\",\n",
        "    \"food\":      \"#F4A261\",\n",
        "    \"tech\":      \"#6BCB77\",\n",
        "    \"sports\":    \"#9B5DE5\",\n",
        "    \"emotions\":  \"#FF6B9D\",\n",
        "    \"countries\": \"#00B4D8\",\n",
        "    \"science\":   \"#E9C46A\",\n",
        "}\n",
        "\n",
        "pca     = PCA(n_components=2, random_state=42)\n",
        "coords  = pca.fit_transform(vectors)\n",
        "var_exp = pca.explained_variance_ratio_\n",
        "\n",
        "@widgets.interact(\n",
        "    cats=widgets.SelectMultiple(\n",
        "        options=sorted(cat_palette.keys()),\n",
        "        value=tuple(sorted(cat_palette.keys())),\n",
        "        description=\"Categories:\",\n",
        "        layout=widgets.Layout(height=\"160px\", width=\"220px\"),\n",
        "    ),\n",
        "    show_labels=widgets.Checkbox(value=True, description=\"Show word labels\"),\n",
        ")\n",
        "def plot_word_map(cats, show_labels):\n",
        "    fig, ax = plt.subplots(figsize=(11, 8))\n",
        "\n",
        "    for cat in cats:\n",
        "        mask = np.array([c == cat for c in categories])\n",
        "        ax.scatter(coords[mask, 0], coords[mask, 1],\n",
        "                   c=cat_palette[cat], s=100, label=cat.capitalize(),\n",
        "                   edgecolors=\"white\", linewidths=0.8, zorder=3)\n",
        "        if show_labels:\n",
        "            for i, (x, y) in enumerate(coords[mask]):\n",
        "                w = np.array(words)[mask][i]\n",
        "                ax.text(x + 0.015, y, w, fontsize=8.5, color=cat_palette[cat],\n",
        "                        path_effects=[pe.withStroke(linewidth=2, foreground=\"white\")])\n",
        "\n",
        "    ax.set_xlabel(f\"PC 1 ({var_exp[0]*100:.1f}% variance)\", fontsize=11)\n",
        "    ax.set_ylabel(f\"PC 2 ({var_exp[1]*100:.1f}% variance)\", fontsize=11)\n",
        "    ax.set_title(\"Word Embedding Map â€” 50 Dimensions â†’ 2D via PCA\",\n",
        "                 fontsize=14, weight=\"bold\")\n",
        "    ax.legend(title=\"Category\", bbox_to_anchor=(1.01, 1), loc=\"upper left\",\n",
        "              fontsize=10, title_fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    sns.despine()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0b68aaa",
      "metadata": {},
      "source": [
        "## The Word Arithmetic\n",
        "\n",
        "One of the most surprising properties of word vectors:\n",
        "\n",
        "$$\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}$$\n",
        "\n",
        "You can do **arithmetic with meanings**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d83d913f",
      "metadata": {},
      "outputs": [],
      "source": [
        "royalty_words  = [w for w, c in zip(words, categories) if c == \"royalty\"]\n",
        "\n",
        "@widgets.interact(\n",
        "    a=widgets.Dropdown(options=royalty_words,  value=\"queen\",   description=\"Word A:\",\n",
        "                       layout=widgets.Layout(width=\"200px\")),\n",
        "    b=widgets.Dropdown(options=royalty_words,  value=\"king\",  description=\"- Word B:\",\n",
        "                       layout=widgets.Layout(width=\"200px\")),\n",
        "    c=widgets.Dropdown(options=royalty_words,  value=\"prince\", description=\"+ Word C:\",\n",
        "                       layout=widgets.Layout(width=\"200px\")),\n",
        ")\n",
        "def word_arithmetic(a, b, c):\n",
        "    result_vec = get_vector(a) - get_vector(b) + get_vector(c)\n",
        "    result_vec /= np.linalg.norm(result_vec)\n",
        "\n",
        "    sims = [(w, cosine_similarity(result_vec, vectors[i]))\n",
        "            for i, w in enumerate(words) if w not in [a, b, c]]\n",
        "    top5 = sorted(sims, key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "    print(f'  \"{a}\" - \"{b}\" + \"{c}\" = ?')\n",
        "    print()\n",
        "    for rank, (w, s) in enumerate(top5, 1):\n",
        "        bar = \"â–ˆ\" * int(s * 30)\n",
        "        print(f\"  {rank}. {w:<15} similarity: {s:.3f}  {bar}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48489af6",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Words are numbers.** Every word in a language model is represented as a list of numbers (over 4000 for large models).\n",
        "2. **Similar meanings â†’ similar vectors.** The model learns this by reading billions of sentences.\n",
        "3. **PCA** lets us squish high-dimensional data into 2D so humans can see the structure.\n",
        "4. **Word arithmetic** (king âˆ’ man + woman â‰ˆ queen) shows that vector spaces capture real-world relationships.\n",
        "\n",
        "This is the core idea behind **every large language model** â€” GPT-5, Gemini, Claude. They all start with word embeddings.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
