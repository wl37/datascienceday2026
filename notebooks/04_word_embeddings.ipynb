{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8c3e15bf",
      "metadata": {},
      "source": [
        "# ðŸ—£ï¸ Word Embeddings\n",
        "## *Teaching computers what words mean*\n",
        "\n",
        "---\n",
        "\n",
        "How does ChatGPT know that **\"dog\"** and **\"puppy\"** are similar?\n",
        "\n",
        "Or that **\"Paris\"** is to **\"France\"** as **\"Tokyo\"** is to **\"Japan\"**?\n",
        "\n",
        "The answer: **word embeddings** â€” representing every word as a list of numbers (a *vector*) so that words with similar meanings end up close together in space.\n",
        "\n",
        "```\n",
        "king   â†’ [0.32, -0.51, 0.78, 0.14, ...300 numbers...]\n",
        "queen  â†’ [0.30, -0.48, 0.75, 0.19, ...]\n",
        "dog    â†’ [-0.62, 0.33, -0.10, 0.88, ...]\n",
        "```\n",
        "\n",
        "This is the **foundation of all modern AI language models** â€” ChatGPT, Gemini, Claude, all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2a53833e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded FastText vectors: 3,000 words, 300 dimensions.\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as pe\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "sns.set_theme(style=\"ticks\", font_scale=1.15)\n",
        "plt.ioff()\n",
        "\n",
        "data_dir = Path(\"../data\")\n",
        "zip_path = data_dir / \"wiki-news-300d-1M.vec.zip\"\n",
        "vec_path = data_dir / \"wiki-news-300d-1M.vec\"\n",
        "url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\n",
        "\n",
        "if not zip_path.exists():\n",
        "    print(\"Downloading FastText vectors (first run only, large file)...\")\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "if not vec_path.exists():\n",
        "    print(\"Extracting .vec file...\")\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extract(\"wiki-news-300d-1M.vec\", data_dir)\n",
        "\n",
        "# Use a practical limit for live demos; set to None to load the full 1M vocabulary.\n",
        "LOAD_LIMIT = 3_000\n",
        "ft = KeyedVectors.load_word2vec_format(vec_path, binary=False, limit=LOAD_LIMIT)\n",
        "\n",
        "print(f\"Loaded FastText vectors: {len(ft):,} words, {ft.vector_size} dimensions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac6ecbf1",
      "metadata": {},
      "source": [
        "## What is a Vector?\n",
        "\n",
        "Think of it as **coordinates** for meaning.\n",
        "\n",
        "In 2D, we can place words on a map. In 50D, we can capture much richer relationships.\n",
        "\n",
        "**Cosine similarity** measures how *similar* two word-vectors point:\n",
        "- Score **1.0** = identical direction (same meaning)\n",
        "- Score **0.0** = perpendicular (unrelated)\n",
        "- Score **-1.0** = opposite direction (antonyms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "58f9e77d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:\n",
        "    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
        "\n",
        "\n",
        "def get_vector(word: str) -> np.ndarray:\n",
        "    return ft[word]\n",
        "\n",
        "\n",
        "def nearest_neighbors(word: str, n: int = 5) -> list[tuple[str, float]]:\n",
        "    return ft.most_similar(word, topn=n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3f93b15",
      "metadata": {},
      "source": [
        "Pick a word and see which other words are most similar to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "8a7bb6d4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee077f869be84ff786655099adbd50f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='Word:', layout=Layout(width='260px'), options=('king', 'queen', 'dâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "demo_words = [\n",
        "    \"king\", \"queen\", \"doctor\", \"nurse\", \"dog\", \"cat\", \"lion\", \"python\",\n",
        "    \"computer\", \"internet\", \"music\", \"guitar\", \"happy\", \"sad\", \"city\", \"country\"\n",
        "]\n",
        "\n",
        "@widgets.interact(\n",
        "    word=widgets.Dropdown(options=demo_words, value=\"king\", description=\"Word:\",\n",
        "                          layout=widgets.Layout(width=\"260px\")),\n",
        "    n=widgets.IntSlider(value=8, min=3, max=12, step=1, description=\"Show top:\",\n",
        "                        style={\"description_width\": \"initial\"},\n",
        "                        layout=widgets.Layout(width=\"300px\")),\n",
        ")\n",
        "def show_neighbors(word, n):\n",
        "\n",
        "    neighbors = nearest_neighbors(word, n)\n",
        "    neighbor_words = [w for w, _ in neighbors]\n",
        "    neighbor_scores = [s for _, s in neighbors]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8.5, max(2.2, n * 0.4)))\n",
        "    colors = plt.cm.RdYlGn(np.clip(neighbor_scores, 0, 1))\n",
        "\n",
        "    bars = ax.barh(range(n), neighbor_scores, color=colors, edgecolor=\"white\", linewidth=1)\n",
        "    ax.set_yticks(range(n))\n",
        "    ax.set_yticklabels(neighbor_words, fontsize=11)\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_xlim(0.4, 1.0)\n",
        "    ax.set_xlabel(\"Cosine Similarity\")\n",
        "    ax.set_title(f'FastText nearest neighbors for \"{word}\"', fontsize=14, weight=\"bold\")\n",
        "    ax.bar_label(bars, fmt=\"%.3f\", padding=4, fontsize=10)\n",
        "\n",
        "    # sns.despine(left=True, bottom=True)\n",
        "    plt.tight_layout()\n",
        "    display(fig)\n",
        "    plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24b216a5",
      "metadata": {},
      "source": [
        "FastText vectors are **300-dimensional**, so we still need dimensionality reduction to visualize them.\n",
        "\n",
        "We use **PCA** to project carefully chosen semantic groups into 2D.\n",
        "Even after projection, related concepts should stay close together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "9e3247fb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbc5b16f230046de82b75fa73d08a73b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(SelectMultiple(description='Groups:', index=(0, 1, 2, 3, 4, 5), layout=Layout(height='16â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "word_groups = {\n",
        "    \"royalty\": [\"king\", \"queen\", \"prince\", \"princess\", \"duke\", \"duchess\", \"throne\", \"crown\"],\n",
        "    \"countries\": [\"france\", \"japan\", \"china\", \"germany\", \"italy\", \"canada\", \"brazil\", \"india\"],\n",
        "    \"capitals\": [\"paris\", \"tokyo\", \"beijing\", \"berlin\", \"rome\", \"ottawa\", \"brasilia\", \"delhi\"],\n",
        "    \"technology\": [\"computer\", \"internet\", \"software\", \"hardware\", \"python\", \"java\", \"database\", \"algorithm\"],\n",
        "    \"animals\": [\"dog\", \"cat\", \"lion\", \"tiger\", \"wolf\", \"bear\", \"eagle\", \"shark\"],\n",
        "    \"emotions\": [\"happy\", \"sad\", \"angry\", \"calm\", \"joy\", \"fear\", \"love\", \"stress\"],\n",
        "}\n",
        "\n",
        "cat_palette = {\n",
        "    \"royalty\": \"#E8575A\",\n",
        "    \"countries\": \"#00B4D8\",\n",
        "    \"capitals\": \"#9B5DE5\",\n",
        "    \"technology\": \"#6BCB77\",\n",
        "    \"animals\": \"#5B8FB9\",\n",
        "    \"emotions\": \"#FF6B9D\",\n",
        "}\n",
        "\n",
        "# Keep only words that were loaded into the current FastText vocabulary limit.\n",
        "word_groups = {\n",
        "    group: [w for w in words if w in ft.key_to_index]\n",
        "    for group, words in word_groups.items()\n",
        "}\n",
        "\n",
        "@widgets.interact(\n",
        "    cats=widgets.SelectMultiple(\n",
        "        options=sorted(word_groups.keys()),\n",
        "        value=tuple(sorted(word_groups.keys())),\n",
        "        description=\"Groups:\",\n",
        "        layout=widgets.Layout(height=\"160px\", width=\"230px\"),\n",
        "    ),\n",
        "    show_labels=widgets.Checkbox(value=True, description=\"Show word labels\"),\n",
        ")\n",
        "def plot_word_map(cats, show_labels):\n",
        "    selected_words = []\n",
        "    selected_cats = []\n",
        "    for cat in cats:\n",
        "        for word in word_groups[cat]:\n",
        "            selected_words.append(word)\n",
        "            selected_cats.append(cat)\n",
        "\n",
        "    if len(selected_words) < 3:\n",
        "        print(\"Please select at least one group with 3+ words in vocabulary.\")\n",
        "        return\n",
        "\n",
        "    vectors = np.vstack([get_vector(w) for w in selected_words])\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    coords = pca.fit_transform(vectors)\n",
        "    var_exp = pca.explained_variance_ratio_\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(11, 8))\n",
        "\n",
        "    for cat in cats:\n",
        "        mask = np.array([c == cat for c in selected_cats])\n",
        "        ax.scatter(\n",
        "            coords[mask, 0], coords[mask, 1],\n",
        "            c=cat_palette[cat], s=120, label=cat.capitalize(),\n",
        "            edgecolors=\"white\", linewidths=0.9, zorder=3, alpha=0.9,\n",
        "        )\n",
        "\n",
        "        if show_labels:\n",
        "            for i, (x, y) in enumerate(coords[mask]):\n",
        "                w = np.array(selected_words)[mask][i]\n",
        "                ax.text(\n",
        "                    x + 0.02, y, w, fontsize=8.5, color=cat_palette[cat],\n",
        "                    path_effects=[pe.withStroke(linewidth=2, foreground=\"white\")],\n",
        "                )\n",
        "\n",
        "    ax.set_xlabel(f\"PC 1 ({var_exp[0]*100:.1f}% variance)\", fontsize=11)\n",
        "    ax.set_ylabel(f\"PC 2 ({var_exp[1]*100:.1f}% variance)\", fontsize=11)\n",
        "    ax.set_title(\"FastText Word Map â€” 300D to 2D via PCA\", fontsize=14, weight=\"bold\")\n",
        "    ax.legend(title=\"Group\", bbox_to_anchor=(1.01, 1), loc=\"upper left\", fontsize=10, title_fontsize=11)\n",
        "    ax.grid(True, alpha=0.25)\n",
        "    sns.despine()\n",
        "    plt.tight_layout()\n",
        "    display(fig)\n",
        "    plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0b68aaa",
      "metadata": {},
      "source": [
        "## The Word Arithmetic\n",
        "\n",
        "FastText also supports analogy-style arithmetic via vector operations, for example:\n",
        "\n",
        "- `paris - france + japan â‰ˆ tokyo`\n",
        "- `walk - walking + swimming â‰ˆ swim`\n",
        "- `good - bad + terrible â‰ˆ awful`\n",
        "\n",
        "These relationships are learned from context in large text corpora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d83d913f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf7e9cac48844d3789c9e5231ce6efc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='Example:', layout=Layout(width='320px'), options=('Gender relationâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "manual_candidates = {\n",
        "    \"Capital transfer\": (\"paris\", \"france\", \"japan\"),\n",
        "    \"Country transfer\": (\"tokyo\", \"japan\", \"france\"),\n",
        "    \"Gender relation\": (\"king\", \"man\", \"woman\"),\n",
        "    \"Royal gender\": (\"queen\", \"woman\", \"man\"),\n",
        "    \"Verb morphology\": (\"walking\", \"walk\", \"swim\"),\n",
        "    \"Adjective relation\": (\"good\", \"bad\", \"terrible\"),\n",
        "    \"Plural form\": (\"people\", \"person\", \"child\"),\n",
        "}\n",
        "\n",
        "\n",
        "def in_vocab(*words: str) -> bool:\n",
        "    return all(w in ft.key_to_index for w in words)\n",
        "\n",
        "\n",
        "analogy_examples = {\n",
        "    name: triple for name, triple in manual_candidates.items()\n",
        "    if in_vocab(*triple)\n",
        "}\n",
        "\n",
        "# Auto-add robust morphology analogies from words guaranteed to be loaded.\n",
        "# Form: \"X-ing relation\" uses X+ing - X + Y -> Y+ing.\n",
        "vocab_words = [w for w in ft.key_to_index if w.isalpha()]\n",
        "\n",
        "\n",
        "ing_pairs = []\n",
        "for base in vocab_words:\n",
        "    if len(base) >= 3:\n",
        "        ing = f\"{base}ing\"\n",
        "        if ing in ft.key_to_index:\n",
        "            ing_pairs.append((base, ing))\n",
        "\n",
        "ing_pairs = sorted(ing_pairs, key=lambda p: ft.key_to_index[p[0]])\n",
        "\n",
        "for i in range(min(4, len(ing_pairs) - 1)):\n",
        "    b1, i1 = ing_pairs[i]\n",
        "    b2, _ = ing_pairs[i + 1]\n",
        "    name = f\"Progressive verb {i + 1}\"\n",
        "    analogy_examples[name] = (i1, b1, b2)\n",
        "\n",
        "\n",
        "plural_pairs = []\n",
        "for base in vocab_words:\n",
        "    if len(base) >= 3:\n",
        "        plural = f\"{base}s\"\n",
        "        if plural in ft.key_to_index:\n",
        "            plural_pairs.append((base, plural))\n",
        "\n",
        "plural_pairs = sorted(plural_pairs, key=lambda p: ft.key_to_index[p[0]])\n",
        "\n",
        "for i in range(min(3, len(plural_pairs) - 1)):\n",
        "    b1, p1 = plural_pairs[i]\n",
        "    b2, _ = plural_pairs[i + 1]\n",
        "    name = f\"Plural noun {i + 1}\"\n",
        "    analogy_examples[name] = (p1, b1, b2)\n",
        "\n",
        "\n",
        "if not analogy_examples:\n",
        "    print(\n",
        "        \"No analogy examples available with the current vocabulary limit \"\n",
        "        f\"(LOAD_LIMIT={LOAD_LIMIT:,}). Increase the limit in the setup cell.\"\n",
        "    )\n",
        "else:\n",
        "    options = list(analogy_examples.keys())\n",
        "\n",
        "    @widgets.interact(\n",
        "        example=widgets.Dropdown(\n",
        "            options=options,\n",
        "            value=options[0],\n",
        "            description=\"Example:\",\n",
        "            layout=widgets.Layout(width=\"320px\"),\n",
        "        ),\n",
        "        topn=widgets.IntSlider(\n",
        "            value=8,\n",
        "            min=3,\n",
        "            max=12,\n",
        "            step=1,\n",
        "            description=\"Show top:\",\n",
        "            style={\"description_width\": \"initial\"},\n",
        "            layout=widgets.Layout(width=\"300px\"),\n",
        "        ),\n",
        "    )\n",
        "    def word_arithmetic(example, topn):\n",
        "        a, b, c = analogy_examples[example]\n",
        "\n",
        "        top = ft.most_similar(positive=[a, c], negative=[b], topn=topn)\n",
        "        result_vec = get_vector(a) - get_vector(b) + get_vector(c)\n",
        "\n",
        "        best_word, _ = top[0]\n",
        "        manual_score = cosine_similarity(result_vec, get_vector(best_word))\n",
        "\n",
        "        print(f'\"{a}\" - \"{b}\" + \"{c}\" = ?')\n",
        "        print(f\"Top prediction: {best_word} (manual cosine to result vector: {manual_score:.3f})\")\n",
        "\n",
        "        labels = [w for w, _ in top]\n",
        "        scores = [s for _, s in top]\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8.5, max(3.2, topn * 0.48)))\n",
        "        bars = ax.barh(range(topn), scores, color=plt.cm.viridis(scores), edgecolor=\"white\")\n",
        "        ax.set_yticks(range(topn))\n",
        "        ax.set_yticklabels(labels, fontsize=11)\n",
        "        ax.invert_yaxis()\n",
        "        ax.set_xlim(0.35, 1.0)\n",
        "        ax.set_xlabel(\"Cosine Similarity\")\n",
        "        ax.set_title(f'Analogy Results: \"{a}\" - \"{b}\" + \"{c}\"', fontsize=13.5, weight=\"bold\")\n",
        "        ax.bar_label(bars, fmt=\"%.3f\", padding=4, fontsize=10)\n",
        "\n",
        "        sns.despine(left=True, bottom=True)\n",
        "        plt.tight_layout()\n",
        "        display(fig)\n",
        "        plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48489af6",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Words are vectors.** Here we used pretrained **FastText 300D embeddings**.\n",
        "2. **Similar meanings â†’ nearby vectors.** We measured this with cosine similarity.\n",
        "3. **Gensim APIs** make practical tasks easy (`most_similar`, analogy queries).\n",
        "4. **PCA maps** help humans inspect structure in high-dimensional spaces.\n",
        "5. **Word arithmetic** can reveal semantic and grammatical relations.\n",
        "\n",
        "This embedding intuition still underpins modern language models, even though production systems now use richer contextual representations.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
